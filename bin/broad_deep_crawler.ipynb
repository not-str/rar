{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "from urllib.parse import urljoin \n",
    "def crawl(seed_url, max_pages=100): \n",
    "   visited_urls = set() \n",
    "   urls_to_visit = [seed_url] \n",
    "   while urls_to_visit and len(visited_urls) < max_pages: \n",
    "       url = urls_to_visit.pop(0)  # Get the next URL \n",
    "       if url in visited_urls: \n",
    "           continue \n",
    "       print(f'Crawling: {url}')  \n",
    "       try: \n",
    "           response = requests.get(url) \n",
    "           response.raise_for_status()  \n",
    "           visited_urls.add(url) \n",
    "           \n",
    "           soup = BeautifulSoup(response.text, 'html.parser') \n",
    "           for link in soup.find_all('a', href=True): \n",
    "               full_url = urljoin(url, link['href'])  \n",
    "               if full_url not in visited_urls: \n",
    "                  urls_to_visit.append(full_url)             \n",
    "       except requests.RequestException as e: \n",
    "           print(f'Error fetching {url}: {e}') \n",
    "# Start crawling from a seed URL \n",
    "crawl('https://siesascs.edu.in/faculty/degree_college.php')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
