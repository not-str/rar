{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    " \n",
    "image_url = \"https://imgs.search.brave.com/ZZArR465yVqKKp0PoFj9gIkdQ_heY5mdbLURRSL0Rao/rs:fit:500:0:0:0/g:ce/aHR0cHM6Ly9tZWRp/YS5pc3RvY2twaG90/by5jb20vaWQvMTU3/NTkwMDQwL3Bob3Rv/L2RvbWVzdGljLWNh/dC1qdW1waW5nLmpw/Zz9zPTYxMng2MTIm/dz0wJms9MjAmYz1W/UWFNa1U4TXNkb3Jx/bTMtQWp0TFc5VG1Y/Nnc3VThxSUpocmhv/c2ZGbUFZPQ\" \n",
    "output_filename = \"gfg_logo.png\" \n",
    "# Send a GET request to fetch the image \n",
    "response = requests.get(image_url) \n",
    "# Save the image data to a local file if the request is successful \n",
    "if response.status_code == 200: \n",
    "   with open(output_filename, \"wb\") as file: \n",
    "       file.write(response.content) \n",
    "       print(f\"Image downloaded successfully as {output_filename}\") \n",
    "else: \n",
    "   print(\"Failed to download the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "# Define the URL of the website to scrape \n",
    "URL = \"https://www.geeksforgeeks.org/\" \n",
    "# Send a GET request to the specified URL and store the response \n",
    "resp = requests.get(URL) \n",
    "# Print the HTTP status code and the HTML content of the response \n",
    "print(\"Status Code:\", resp.status_code) \n",
    "print(\"\\nResponse Content:\") \n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "def crawl(url): \n",
    "   response = requests.get(url) \n",
    "   soup = BeautifulSoup(response.text, 'html.parser') \n",
    "   return [link.get('href') for link in soup.find_all('a', href=True) if \n",
    "link.get('href').startswith('http')] \n",
    "links = crawl('https://siesascs.edu.in')  # Replace with the URL you want to crawl \n",
    "print(links)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
